{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pandasql as ps\n",
    "from skimpy import skim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation de l'appareil : mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Utilisation de l'appareil : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_sales_train='/Users/arthurgaujoux/Desktop/Farpoint/predict_future_sales/df_agg_sales_train.csv'\n",
    "data_pd=pd.read_csv(\n",
    "    agg_sales_train,\n",
    "    dtype={\n",
    "        'date': 'string',\n",
    "        'shop_id': 'int64',\n",
    "        'item_id': 'int64',\n",
    "        'rescaling_item_cnt_day': 'int64',\n",
    "        'monday_flag': 'int64',\n",
    "        'tuesday_flag': 'int64',\n",
    "        'wednesday_flag': 'int64',\n",
    "        'thursday_flag': 'int64',\n",
    "        'friday_flag': 'int64',\n",
    "        'saturday_flag': 'int64',\n",
    "        'sunday_flag': 'int64',\n",
    "        'week_end_flag': 'int64',\n",
    "        'january_flag': 'int64',\n",
    "        'february_flag': 'int64',\n",
    "        'march_flag': 'int64',\n",
    "        'april_flag': 'int64',\n",
    "        'may_flag': 'int64',\n",
    "        'june_flag': 'int64',\n",
    "        'july_flag': 'int64',\n",
    "        'august_flag': 'int64',\n",
    "        'september_flag': 'int64',\n",
    "        'october_flag': 'int64',\n",
    "        'november_flag': 'int64',\n",
    "        'december_flag': 'int64',\n",
    "        'holiday_flag': 'int64'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              date  shop_id  item_id  rescaling_item_cnt_day  monday_flag  \\\n",
      "0       2013-01-01        2      991                      23            0   \n",
      "1       2013-01-01        2     1472                      23            0   \n",
      "2       2013-01-01        2     1905                      23            0   \n",
      "3       2013-01-01        2     2920                      24            0   \n",
      "4       2013-01-01        2     3320                      23            0   \n",
      "...            ...      ...      ...                     ...          ...   \n",
      "631916  2013-06-30       59    18755                      23            0   \n",
      "631917  2013-06-30       59    19864                      23            0   \n",
      "631918  2013-06-30       59    20949                      23            0   \n",
      "631919  2013-06-30       59    21487                      24            0   \n",
      "631920  2013-06-30       59    22087                      26            0   \n",
      "\n",
      "        tuesday_flag  wednesday_flag  thursday_flag  friday_flag  \\\n",
      "0                  1               0              0            0   \n",
      "1                  1               0              0            0   \n",
      "2                  1               0              0            0   \n",
      "3                  1               0              0            0   \n",
      "4                  1               0              0            0   \n",
      "...              ...             ...            ...          ...   \n",
      "631916             0               0              0            0   \n",
      "631917             0               0              0            0   \n",
      "631918             0               0              0            0   \n",
      "631919             0               0              0            0   \n",
      "631920             0               0              0            0   \n",
      "\n",
      "        saturday_flag  ...  april_flag  may_flag  june_flag  july_flag  \\\n",
      "0                   0  ...           0         0          0          0   \n",
      "1                   0  ...           0         0          0          0   \n",
      "2                   0  ...           0         0          0          0   \n",
      "3                   0  ...           0         0          0          0   \n",
      "4                   0  ...           0         0          0          0   \n",
      "...               ...  ...         ...       ...        ...        ...   \n",
      "631916              0  ...           0         0          0          0   \n",
      "631917              0  ...           0         0          0          0   \n",
      "631918              0  ...           0         0          0          0   \n",
      "631919              0  ...           0         0          0          0   \n",
      "631920              0  ...           0         0          0          0   \n",
      "\n",
      "        august_flag  september_flag  october_flag  november_flag  \\\n",
      "0                 0               0             0              0   \n",
      "1                 0               0             0              0   \n",
      "2                 0               0             0              0   \n",
      "3                 0               0             0              0   \n",
      "4                 0               0             0              0   \n",
      "...             ...             ...           ...            ...   \n",
      "631916            0               0             0              0   \n",
      "631917            0               0             0              0   \n",
      "631918            0               0             0              0   \n",
      "631919            0               0             0              0   \n",
      "631920            0               0             0              0   \n",
      "\n",
      "        december_flag  holiday_flag  \n",
      "0                   0             1  \n",
      "1                   0             1  \n",
      "2                   0             1  \n",
      "3                   0             1  \n",
      "4                   0             1  \n",
      "...               ...           ...  \n",
      "631916              0             0  \n",
      "631917              0             0  \n",
      "631918              0             0  \n",
      "631919              0             0  \n",
      "631920              0             0  \n",
      "\n",
      "[631921 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "#################### sampling data ###################\n",
    "query = \"\"\" \n",
    "select\n",
    "    date,\n",
    "    shop_id,\n",
    "    item_id,\n",
    "    rescaling_item_cnt_day,\n",
    "    monday_flag,\n",
    "    tuesday_flag,\n",
    "    wednesday_flag,\n",
    "    thursday_flag,\n",
    "    friday_flag, \n",
    "    saturday_flag, \n",
    "    sunday_flag, \n",
    "    week_end_flag, \n",
    "    january_flag, \n",
    "    february_flag, \n",
    "    march_flag, \n",
    "    april_flag, \n",
    "    may_flag, \n",
    "    june_flag, \n",
    "    july_flag, \n",
    "    august_flag, \n",
    "    september_flag,\n",
    "    october_flag,\n",
    "    november_flag,\n",
    "    december_flag,\n",
    "    holiday_flag\n",
    "from data_pd\n",
    "where date >= '2013-01-01'\n",
    "and date <='2013-06-30'\n",
    "\"\"\"\n",
    "sample_data_pd = ps.sqldf(query, locals())\n",
    "\n",
    "print(sample_data_pd)\n",
    "\n",
    "sample_data_pd=sample_data_pd[sample_data_pd['rescaling_item_cnt_day']<100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n",
       "│ <span style=\"font-style: italic\">         Data Summary          </span> <span style=\"font-style: italic\">      Data Types       </span>                                                         │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                         │\n",
       "│ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Dataframe         </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Values  </span>┃ ┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Column Type </span>┃<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\"> Count </span>┃                                                         │\n",
       "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                         │\n",
       "│ │ Number of rows    │ 2935608 │ │ int64       │ 3     │                                                         │\n",
       "│ │ Number of columns │ 3       │ └─────────────┴───────┘                                                         │\n",
       "│ └───────────────────┴─────────┘                                                                                 │\n",
       "│ <span style=\"font-style: italic\">                                                    number                                                    </span>  │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓  │\n",
       "│ ┃<span style=\"font-weight: bold\"> column                   </span>┃<span style=\"font-weight: bold\"> NA  </span>┃<span style=\"font-weight: bold\"> NA %  </span>┃<span style=\"font-weight: bold\"> mean    </span>┃<span style=\"font-weight: bold\"> sd     </span>┃<span style=\"font-weight: bold\"> p0  </span>┃<span style=\"font-weight: bold\"> p25   </span>┃<span style=\"font-weight: bold\"> p50   </span>┃<span style=\"font-weight: bold\"> p75    </span>┃<span style=\"font-weight: bold\"> p100   </span>┃<span style=\"font-weight: bold\"> hist   </span>┃  │\n",
       "│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩  │\n",
       "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">shop_id                 </span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">     33</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 16.23</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   22</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   31</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    47</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    59</span> │ <span style=\"color: #008000; text-decoration-color: #008000\">▃▄█▅▅█</span> │  │\n",
       "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">item_id                 </span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  10200</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  6324</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 4476</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 9343</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 15680</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 22170</span> │ <span style=\"color: #008000; text-decoration-color: #008000\">▇█▅▅▅▅</span> │  │\n",
       "│ │ <span style=\"color: #af87ff; text-decoration-color: #af87ff\">rescaling_item_cnt_day  </span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  23.23</span> │ <span style=\"color: #008080; text-decoration-color: #008080\"> 1.445</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">  0</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   23</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">   23</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    23</span> │ <span style=\"color: #008080; text-decoration-color: #008080\">    99</span> │ <span style=\"color: #008000; text-decoration-color: #008000\">   █  </span> │  │\n",
       "│ └──────────────────────────┴─────┴───────┴─────────┴────────┴─────┴───────┴───────┴────────┴────────┴────────┘  │\n",
       "╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────────────────────── skimpy summary ─────────────────────────────────────────────────╮\n",
       "│ \u001b[3m         Data Summary          \u001b[0m \u001b[3m      Data Types       \u001b[0m                                                         │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓ ┏━━━━━━━━━━━━━┳━━━━━━━┓                                                         │\n",
       "│ ┃\u001b[1;36m \u001b[0m\u001b[1;36mDataframe        \u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mValues \u001b[0m\u001b[1;36m \u001b[0m┃ ┃\u001b[1;36m \u001b[0m\u001b[1;36mColumn Type\u001b[0m\u001b[1;36m \u001b[0m┃\u001b[1;36m \u001b[0m\u001b[1;36mCount\u001b[0m\u001b[1;36m \u001b[0m┃                                                         │\n",
       "│ ┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩ ┡━━━━━━━━━━━━━╇━━━━━━━┩                                                         │\n",
       "│ │ Number of rows    │ 2935608 │ │ int64       │ 3     │                                                         │\n",
       "│ │ Number of columns │ 3       │ └─────────────┴───────┘                                                         │\n",
       "│ └───────────────────┴─────────┘                                                                                 │\n",
       "│ \u001b[3m                                                    number                                                    \u001b[0m  │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓  │\n",
       "│ ┃\u001b[1m \u001b[0m\u001b[1mcolumn                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mNA % \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmean   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1msd    \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp0 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp25  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp50  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp75   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mp100  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mhist  \u001b[0m\u001b[1m \u001b[0m┃  │\n",
       "│ ┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩  │\n",
       "│ │ \u001b[38;5;141mshop_id                 \u001b[0m │ \u001b[36m  0\u001b[0m │ \u001b[36m    0\u001b[0m │ \u001b[36m     33\u001b[0m │ \u001b[36m 16.23\u001b[0m │ \u001b[36m  0\u001b[0m │ \u001b[36m   22\u001b[0m │ \u001b[36m   31\u001b[0m │ \u001b[36m    47\u001b[0m │ \u001b[36m    59\u001b[0m │ \u001b[32m▃▄█▅▅█\u001b[0m │  │\n",
       "│ │ \u001b[38;5;141mitem_id                 \u001b[0m │ \u001b[36m  0\u001b[0m │ \u001b[36m    0\u001b[0m │ \u001b[36m  10200\u001b[0m │ \u001b[36m  6324\u001b[0m │ \u001b[36m  0\u001b[0m │ \u001b[36m 4476\u001b[0m │ \u001b[36m 9343\u001b[0m │ \u001b[36m 15680\u001b[0m │ \u001b[36m 22170\u001b[0m │ \u001b[32m▇█▅▅▅▅\u001b[0m │  │\n",
       "│ │ \u001b[38;5;141mrescaling_item_cnt_day  \u001b[0m │ \u001b[36m  0\u001b[0m │ \u001b[36m    0\u001b[0m │ \u001b[36m  23.23\u001b[0m │ \u001b[36m 1.445\u001b[0m │ \u001b[36m  0\u001b[0m │ \u001b[36m   23\u001b[0m │ \u001b[36m   23\u001b[0m │ \u001b[36m    23\u001b[0m │ \u001b[36m    99\u001b[0m │ \u001b[32m   █  \u001b[0m │  │\n",
       "│ └──────────────────────────┴─────┴───────┴─────────┴────────┴─────┴───────┴───────┴────────┴────────┴────────┘  │\n",
       "╰────────────────────────────────────────────────────── End ──────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_pd=data_pd[data_pd['rescaling_item_cnt_day']<100]\n",
    "skim(data_pd[['shop_id', 'item_id', 'rescaling_item_cnt_day']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_with_flags(data, target_col, sequence_length):\n",
    "    \"\"\"\n",
    "    Crée des séquences avec les identifiants, les flags temporels et les ventes cibles.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Données d'entrée, avec les flags et autres colonnes nécessaires.\n",
    "        target_col (str): Colonne cible (par exemple, 'ct_item_day').\n",
    "        sequence_length (int): Longueur des séquences glissantes.\n",
    "\n",
    "    Returns:\n",
    "        X (np.ndarray): Séquences d'entrée (avec identifiants et flags).\n",
    "        y (np.ndarray): Valeurs cibles (les ventes à prédire).\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    # Trier les données par shop_id, item_id et date pour conserver l'ordre chronologique\n",
    "    data = data.sort_values(by=[\"shop_id\", \"item_id\", \"date\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Grouper les données par shop_id et item_id\n",
    "    grouped = data.groupby([\"shop_id\", \"item_id\"])\n",
    "    \n",
    "    for (shop_id, item_id), group in grouped:\n",
    "        for i in range(len(group) - sequence_length):\n",
    "            # Créer une séquence des valeurs cibles (target_col)\n",
    "            seq = group[target_col].iloc[i:i + sequence_length].values\n",
    "            \n",
    "            # Sélectionner les flags temporels pour chaque point dans la séquence\n",
    "            seq_flags = group[[\n",
    "                'monday_flag', 'tuesday_flag', 'wednesday_flag', 'thursday_flag', 'friday_flag', \n",
    "                'saturday_flag', 'sunday_flag', 'week_end_flag', \n",
    "                'january_flag', 'february_flag', 'march_flag', 'april_flag', 'may_flag', \n",
    "                'june_flag', 'july_flag', 'august_flag', 'september_flag', 'october_flag', \n",
    "                'november_flag', 'december_flag', 'holiday_flag'\n",
    "            ]].iloc[i:i + sequence_length].values\n",
    "\n",
    "            # Ajouter les identifiants (shop_id, item_id) à chaque point de la séquence\n",
    "            seq_with_flags = np.hstack((np.repeat([[shop_id, item_id]], sequence_length, axis=0), \n",
    "                                        seq_flags))  # Ajoute les identifiants avec les flags\n",
    "            \n",
    "            # Ajouter les ventes cibles à la fin de la séquence\n",
    "            seq_with_target = np.hstack((seq_with_flags, seq.reshape(-1, 1)))  # Ajoute les ventes (target_col)\n",
    "            \n",
    "            # La cible pour la prochaine valeur après la séquence\n",
    "            target = group[target_col].iloc[i + sequence_length]\n",
    "            \n",
    "            # Ajouter la séquence et la cible à la liste\n",
    "            X.append(seq_with_target)\n",
    "            y.append(target)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, lstm_hidden_size, num_layers, nhead):\n",
    "        super(TransformerLSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, lstm_hidden_size, num_layers, batch_first=True)\n",
    "        self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=lstm_hidden_size, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(lstm_hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        transformer_out = self.transformer_encoder(lstm_out.permute(1, 0, 2))\n",
    "        out = self.fc(self.relu(transformer_out[:, -1, :]))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "        sequence_length,\n",
    "        input_size,\n",
    "        lstm_hidden_size,\n",
    "        num_layers,\n",
    "        nhead,\n",
    "        learning_rate,\n",
    "        batch_size\n",
    "    ):\n",
    "\n",
    "    train_losses = []\n",
    "    validation_accuracies = []\n",
    "    validation_losses = []\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "\n",
    "    # create sequence\n",
    "    X, y = create_sequences_with_flags(data=sample_data_pd, target_col=\"rescaling_item_cnt_day\", sequence_length=sequence_length)\n",
    "\n",
    "    # timne series split\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # rescaling the target\n",
    "    scaler = MinMaxScaler()\n",
    "    y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_val_scaled = scaler.transform(y_val.reshape(-1, 1))\n",
    "\n",
    "    # creating tensor\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).permute(0, 2, 1).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "    model = TransformerLSTMModel(input_size, lstm_hidden_size, num_layers, nhead).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Creating of a dataloader, without mixing (shuffle=False)\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Creating of a dataloader for validation\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # transfer to GPU \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculation of the average loss for the epoch \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Model Validation\n",
    "        model.eval()\n",
    "        val_predictions = []  \n",
    "        val_labels = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                batch_predictions = model(X_batch)\n",
    "                val_predictions.append(batch_predictions.cpu().numpy())\n",
    "                val_labels.append(y_batch.cpu().numpy())\n",
    "        \n",
    "        val_predictions = np.concatenate(val_predictions, axis=0)\n",
    "        val_labels = np.concatenate(val_labels, axis=0)\n",
    "\n",
    "        val_loss = criterion(torch.tensor(val_predictions), torch.tensor(val_labels))\n",
    "        validation_losses.append(val_loss.item())\n",
    "\n",
    "        # Calcul de la précision pour la validation\n",
    "        mae_val = mean_absolute_error(val_labels, val_predictions)\n",
    "        val_accuracy = 100 - (mae_val / np.mean(val_labels) * 100)\n",
    "        validation_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Analysis\n",
    "    print(f\"Validation Loss moyenne: {np.mean(validation_losses):.4f}\")\n",
    "    print(f\"Précision moyenne: {np.mean(validation_accuracies):.2f}%\")\n",
    "    print(f\"Précision de validation à la dernière époque: {validation_accuracies[-1]:.2f}%\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label=\"Train Loss\", color=\"blue\")\n",
    "    plt.plot(range(1, epochs + 1), validation_losses, label=\"Validation Loss\", color=\"red\")\n",
    "    plt.title(\"Train and Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(1, epochs + 1), validation_accuracies, label=\"Validation Accuracy\", color=\"green\")\n",
    "    plt.title(\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Comparaison des Pertes d'Entraînement et de Validation:\n",
    "#Si la perte d'entraînement est beaucoup plus faible que la perte de validation, cela peut indiquer un sur-apprentissage. Cela signifie que le modèle s'adapte trop bien aux données d'entraînement mais ne généralise pas bien aux nouvelles données.\n",
    "#2. Visualisation des Courbes d'Apprentissage:\n",
    "#Tracez les courbes de perte d'entraînement et de validation. Si la courbe de validation commence à augmenter alors que la courbe d'entraînement continue de diminuer, cela peut être un signe de sur-apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import optuna\n",
    "#\n",
    "#def objective(trial):\n",
    "#    lstm_hidden_size = trial.suggest_categorical('lstm_hidden_size', [64, 128, 256])\n",
    "#    nhead = trial.suggest_int('nhead', 2, 8)\n",
    "#    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "#    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "#    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "#    sequence_length = trial.suggest_categorical('sequence_length', [14, 30])\n",
    "#    \n",
    "#    # Code pour entraîner le modèle ici...\n",
    "#    return validation_loss  # Minimise cette valeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arthurgaujoux/venv_farpoint_POC/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 24, got 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlstm_hidden_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 60\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(sequence_length, input_size, lstm_hidden_size, num_layers, nhead, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     58\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# transfer to GPU \u001b[39;00m\n\u001b[1;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 60\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, y_batch)\n\u001b[1;32m     62\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/venv_farpoint_POC/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv_farpoint_POC/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m, in \u001b[0;36mTransformerLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     transformer_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(lstm_out)\n\u001b[1;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(transformer_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]))\n",
      "File \u001b[0;32m~/venv_farpoint_POC/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv_farpoint_POC/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv_farpoint_POC/lib/python3.12/site-packages/torch/nn/modules/rnn.py:898\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    894\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[1;32m    895\u001b[0m                           max_batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    896\u001b[0m                           dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    897\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n\u001b[0;32m--> 898\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "File \u001b[0;32m~/venv_farpoint_POC/lib/python3.12/site-packages/torch/nn/modules/rnn.py:827\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    823\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    824\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    825\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    826\u001b[0m                        ):\n\u001b[0;32m--> 827\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    829\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    831\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/venv_farpoint_POC/lib/python3.12/site-packages/torch/nn/modules/rnn.py:246\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 24, got 7"
     ]
    }
   ],
   "source": [
    "main(\n",
    "    sequence_length = 7,\n",
    "    input_size=24,\n",
    "    lstm_hidden_size = 128,\n",
    "    num_layers = 3,\n",
    "    nhead = 8,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_farpoint_POC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
